#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\bullet 1 0 9 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
ARMA Models
\end_layout

\begin_layout Section
AR(p) Processes
\end_layout

\begin_layout Standard
We have learned Wold decomposition in the previous lecture.
 Let 
\begin_inset Formula $e_{t}$
\end_inset

 be strictly stationary ergodic white noise.
 
\end_layout

\begin_layout Itemize
MA(1)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\mu+e_{t}+\theta e_{t-1}
\]

\end_inset


\end_layout

\begin_layout Standard
mean: 
\begin_inset Formula $E\left[y_{t}\right]=\mu$
\end_inset


\end_layout

\begin_layout Standard
variance: 
\begin_inset Formula $var\left(y_{t}\right)=\theta^{2}+1$
\end_inset


\end_layout

\begin_layout Standard
autocovariance: 
\begin_inset Formula $E\left[e_{t}e_{t-1}\right]=\theta$
\end_inset


\end_layout

\begin_layout Itemize
MA(
\begin_inset Formula $\infty$
\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\mu+\sum_{j=1}^{\infty}b_{j}e_{t-j}
\]

\end_inset

where 
\begin_inset Formula $b_{0}=1$
\end_inset


\end_layout

\begin_layout Itemize
AR(1)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\alpha_{0}+\alpha_{1}y_{t-1}+e_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
mean: 
\begin_inset Formula $E\left[y_{t}\right]=\frac{\alpha_{0}}{1-\alpha_{1}}$
\end_inset


\end_layout

\begin_layout Standard
variance: 
\begin_inset Formula $var\left(y_{t}\right)=\frac{\sigma^{2}}{1-\alpha_{1}^{2}}$
\end_inset


\end_layout

\begin_layout Standard
MA(
\begin_inset Formula $\infty$
\end_inset

) regression: 
\begin_inset Formula $y_{t}=\mu+\sum_{j=1}^{\infty}\alpha_{1}^{j}e_{t-j}$
\end_inset

, where 
\begin_inset Formula $\mu=\frac{\alpha_{0}}{1-\alpha_{1}}$
\end_inset


\end_layout

\begin_layout Standard
lag operator representations:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\left(1-\alpha{\rm L}\right)y_{t} & =\alpha_{0}+e_{t}\\
y_{t} & =\left(1-\alpha{\rm L}\right)^{-1}\left(\alpha_{0}+e_{t}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
For stationary, 
\begin_inset Formula $\left|\alpha\right|<1$
\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\alpha=1$
\end_inset

, unit root.
\end_layout

\begin_layout Itemize
AR(p)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\alpha_{0}+\alpha_{1}y_{t-1}+\alpha_{2}y_{t-2}+...++\alpha_{p}y_{t-p}+e_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
lag operator: 
\begin_inset Formula $\left(1-\alpha\left({\rm L}\right)\right)y_{t}=\alpha_{0}+e_{t}$
\end_inset


\end_layout

\begin_layout Standard
stationary: all roots of 
\begin_inset Formula $1-\alpha\left(z\right)=0$
\end_inset

 are at the side of the unit circle.
\end_layout

\begin_layout Section
Impulse Response
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=b\left({\rm L}\right)e_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
b_{j}=\frac{\partial y_{t+j}}{\partial e_{t}}
\]

\end_inset


\end_layout

\begin_layout Itemize
If the underlying model is AR(p), we do not want to estimate all 
\begin_inset Formula $b_{0},b_{1},...,b_{\infty}$
\end_inset

.
 How to compute the impulse response?
\end_layout

\begin_layout Standard
Recursive representations
\begin_inset Formula 
\begin{align*}
y_{t} & =\alpha_{1}y_{t-1}+\alpha_{2}y_{t-2}+...++\alpha_{p}y_{t-p}+e_{t}\\
y_{t+1} & =\alpha_{1}y_{t}+\alpha_{2}y_{t-1}+...++\alpha_{p}y_{t-p-1}+e_{t+1}\\
y_{t+2} & =\alpha_{1}y_{t+1}+\alpha_{2}y_{t}+...++\alpha_{p}y_{t-p-2}+e_{t+2}\\
\vdots\\
y_{t+p} & =\alpha_{1}y_{t+p-1}+\alpha_{2}y_{t+p-2}+...++\alpha_{p}y_{t}+e_{t+p}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The effect of a shock to 
\begin_inset Formula $e_{t}$
\end_inset

 alone will spread via the dynamic system.
\end_layout

\begin_layout Standard
(1) If 
\begin_inset Formula $\Delta e_{t}=1$
\end_inset

 (normalization), then 
\begin_inset Formula $\Delta y_{t}=1\Rightarrow b_{0}=1$
\end_inset


\end_layout

\begin_layout Standard
(2) 
\begin_inset Formula $\Delta y_{t+1}=\alpha_{1}\Delta y_{t}=\alpha_{1}b_{0}\equiv b_{1}$
\end_inset


\end_layout

\begin_layout Standard
(3) 
\begin_inset Formula $\Delta y_{t+2}=\alpha_{1}\Delta y_{t+1}+\alpha_{2}\Delta y_{t}=\alpha_{1}b_{1}+\alpha_{2}\cdot1\equiv b_{2}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\Delta y_{t+p}=\alpha_{1}b_{p-1}+\alpha_{2}b_{p-2}+...+\alpha_{p}b_{0}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\Delta y_{t+j}=\alpha_{1}b_{j-1}+\alpha_{2}b_{j-2}+...+\alpha_{p}b_{j-p}$
\end_inset


\end_layout

\begin_layout Section
ARMA and ARIMA Processes
\end_layout

\begin_layout Itemize
ARMA: 
\begin_inset Formula $\left(1-\alpha\left({\rm L}\right)\right)y_{t}=b\left({\rm L}\right)e_{t}$
\end_inset


\end_layout

\begin_layout Itemize
ARIMA(p,d,q): 
\begin_inset Formula $\left(1-\alpha\left({\rm L}\right)\right)\left(1-{\rm L}\right)^{d}y_{t}=b\left({\rm L}\right)e_{t}$
\end_inset


\end_layout

\begin_layout Section
Estimation and Asymptotic Distribution
\end_layout

\begin_layout Standard
Estimate AR: take 
\begin_inset Formula $X_{t}=\left(1,y_{t-1},...,y_{t-p}\right)$
\end_inset

, run OLS:
\begin_inset Formula 
\[
\hat{\alpha}=\left(\frac{X'X}{n}\right)^{-1}\frac{X'y}{n}
\]

\end_inset


\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $y_{t}$
\end_inset

 is strictly stationary, ergodic, 
\begin_inset Formula $E\left[y_{t}^{2}\right]<\infty$
\end_inset

, then 
\begin_inset Formula $\hat{\alpha}\overset{p}{\to}\alpha$
\end_inset

 and 
\begin_inset Formula $\hat{\sigma}^{2}\overset{p}{\to}\sigma^{2}$
\end_inset


\end_layout

\begin_layout Standard
Asymptotic normality: If 
\begin_inset Formula $e_{t}$
\end_inset

 is MDS, with 
\begin_inset Formula $\mathscr{F}$
\end_inset

 including 
\begin_inset Formula $X_{t}$
\end_inset

, then 
\begin_inset Formula 
\[
E\left[X_{t}e_{t}\mid\mathscr{F}_{t-1}\right]=X_{t}E\left[e_{t}\mid\mathscr{F}_{t-1}\right]=0
\]

\end_inset


\end_layout

\begin_layout Standard
then 
\begin_inset Formula $\sqrt{n}\left(\hat{\alpha}-\alpha\right)\overset{p}{\to}N\left(0,Q^{-1}\sum Q^{-1}\right)$
\end_inset

,
\end_layout

\begin_layout Standard
where 
\begin_inset Formula $Q=E\left[X_{t}X_{t}'\right]$
\end_inset

 and 
\begin_inset Formula $\sum=E\left[X_{t}X_{t}'e_{t}^{2}\right]$
\end_inset

.
\end_layout

\begin_layout Standard
Under conditional homoskedasticity 
\begin_inset Formula $E\left[e_{t}^{2}\mid\mathscr{F}_{t-1}\right]=\sigma^{2}$
\end_inset

, then the variance is simplified to 
\begin_inset Formula 
\begin{align*}
\sum & =E\left[X_{t}X_{t}'e_{t}^{2}\right]=E\left[X_{t}X_{t}'E\left[e_{t}^{2}\mid\mathscr{F}_{t-1}\right]\right]\\
 & =E\left[X_{t}X_{t}'\right]\sigma^{2}=Q\sigma^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
then 
\begin_inset Formula $\sqrt{n}\left(\hat{\alpha}-\alpha\right)\overset{p}{\to}N\left(0,Q^{-1}\sigma^{2}\right)$
\end_inset


\end_layout

\begin_layout Standard
Without MDS, 
\begin_inset Formula $z_{t}=X_{t}e_{t}$
\end_inset

 can be serially correlated, we need to estimate the long-run variance 
\begin_inset Formula $\varOmega=\sum_{\ell=-\infty}^{\infty}E\left[X_{t}X_{t-\ell}'e_{t}e_{t-\ell}\right]$
\end_inset


\end_layout

\begin_layout Section
Model Selection
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\rm AIC}=\log\hat{\sigma}^{2}+2\frac{p}{n}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\rm BIC}=\log\hat{\sigma}^{2}+\frac{p}{n}\log n
\]

\end_inset


\end_layout

\begin_layout Section
Regression with Time Series Data
\end_layout

\begin_layout Standard
Observe 
\begin_inset Formula $\left(y_{t},X_{t}\right)_{t=1}^{T}$
\end_inset

, want to run regression
\begin_inset Formula 
\[
y_{t}=X_{t}'\beta+e_{t}
\]

\end_inset

where 
\begin_inset Formula $X_{t}$
\end_inset

 can include lagged dependent variables.
\end_layout

\begin_layout Standard
AR(p) 
\end_layout

\begin_layout Standard
By the definition of projection, 
\begin_inset Formula $E\left[X_{t}e_{t}\right]=0$
\end_inset


\end_layout

\begin_layout Standard
The OLS estimator is 
\begin_inset Formula $\hat{\beta}=\left(X'X\right)^{-1}X'y$
\end_inset


\end_layout

\begin_layout Standard
The uncorrelation is necessary for asymptotic normality.
\end_layout

\begin_layout Standard
If we impose MDS, 
\begin_inset Formula $E\left[e_{t}\mid\mathscr{F}_{t-1}\right]=0$
\end_inset

, where 
\begin_inset Formula $\mathscr{F}_{t-1}$
\end_inset

 is adapted to 
\begin_inset Formula $\left(X_{t},e_{t-1}\right)$
\end_inset


\end_layout

\begin_layout Standard
then we have MDS CLT, because
\begin_inset Formula 
\[
E\left[X_{t}e_{t}\mid\mathscr{F}_{t-1}\right]=X_{t}E\left[e_{t}\mid\mathscr{F}_{t-1}\right]=X_{t}\cdot0=0
\]

\end_inset

is also MDS.
\end_layout

\begin_layout Standard
Under MDS 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{p}{\to}N\left(0,Q_{X}^{-1}\sum Q_{X}^{-1}\right)
\]

\end_inset

where 
\begin_inset Formula $\varOmega=E\left[X_{t}X_{t}'e_{t}^{2}\right]$
\end_inset


\end_layout

\begin_layout Standard
Under 
\begin_inset Formula $E\left[X_{t}e_{t}\right]=0$
\end_inset

, we need conditions about the 
\begin_inset Formula $\alpha$
\end_inset

-mixing coefficient, then
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{p}{\to}N\left(0,Q_{X}^{-1}\sum Q_{X}^{-1}\right)
\]

\end_inset

where 
\begin_inset Formula $\varOmega$
\end_inset

 is the long-run variance of 
\begin_inset Formula $\left\{ X_{t}e_{t}\right\} $
\end_inset

.
\end_layout

\begin_layout Section
Regression with Deterministic Trend
\end_layout

\begin_layout Standard
\begin_inset Formula $y_{t}=T_{t}+u_{t}$
\end_inset

, where 
\begin_inset Formula $T_{t}$
\end_inset

 is a deterministic trend and 
\begin_inset Formula $u_{t}$
\end_inset

 is a random error term.
\end_layout

\begin_layout Example
\begin_inset Formula $T_{t}=\beta_{0}+\beta_{1}t$
\end_inset

 (linear trend) or 
\begin_inset Formula $T_{t}=\beta_{0}+\beta_{1}t++\beta_{2}t^{2}$
\end_inset

 (quadratic trend)
\end_layout

\begin_layout Example
Fact: 
\begin_inset Formula 
\[
\frac{1}{n^{1+r}}\sum_{t=1}^{n}t^{r}=\frac{1}{n}\sum_{t=1}^{n}\left(\frac{t}{n}\right)^{r}\to\int_{0}^{1}x^{r}dx=\frac{1}{1+r}x^{r+1}\mid_{0}^{1}=\frac{1}{1+r}
\]

\end_inset


\end_layout

\begin_layout Example
Thus, 
\begin_inset Formula $\frac{1}{n^{2}}\sum_{t=1}^{n}t=\frac{1}{2}$
\end_inset

, 
\begin_inset Formula $\frac{1}{n^{3}}\sum_{t=1}^{n}t^{2}=\frac{1}{3}$
\end_inset


\end_layout

\begin_layout Example
OLS estimator
\begin_inset Formula 
\[
\hat{\beta}-\beta=\left(X'X\right)^{-1}X'u=\left(\begin{array}{cc}
n & \sum_{t=1}^{n}t\\
\sum_{t=1}^{n}t & \sum_{t=1}^{n}t^{2}
\end{array}\right)^{-1}\left(\begin{array}{c}
\sum u_{t}\\
\sum tu_{t}
\end{array}\right)
\]

\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $D_{n}=\left(\begin{array}{cc}
n^{\frac{1}{2}} & 0\\
0 & n^{\frac{3}{2}}
\end{array}\right)$
\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula 
\begin{align*}
D_{n}\left(\hat{\beta}-\beta\right) & =D_{n}\left(\begin{array}{cc}
n & \sum_{t=1}^{n}t\\
\sum_{t=1}^{n}t & \sum_{t=1}^{n}t^{2}
\end{array}\right)^{-1}\left(\begin{array}{c}
\sum u_{t}\\
\sum tu_{t}
\end{array}\right)\\
 & =D_{n}\left(\begin{array}{cc}
n & \sum_{t=1}^{n}t\\
\sum_{t=1}^{n}t & \sum_{t=1}^{n}t^{2}
\end{array}\right)^{-1}D_{n}D_{n}^{-1}\left(\begin{array}{c}
\sum u_{t}\\
\sum tu_{t}
\end{array}\right)\\
 & =\left(D_{n}^{-1}\left(\begin{array}{cc}
n & \sum_{t=1}^{n}t\\
\sum_{t=1}^{n}t & \sum_{t=1}^{n}t^{2}
\end{array}\right)D_{n}^{-1}\right)^{-1}\left(\begin{array}{c}
\frac{1}{\sqrt{n}}\sum u_{t}\\
\frac{1}{n^{3/2}}\sum tu_{t}
\end{array}\right)\\
 & =\left(\begin{array}{cc}
1 & \frac{1}{n^{2}}\sum_{t=1}^{n}t\\
\frac{1}{n^{2}}\sum_{t=1}^{n}t & \frac{1}{n^{3}}\sum_{t=1}^{n}t^{2}
\end{array}\right)^{-1}\left(\begin{array}{c}
\frac{1}{\sqrt{n}}\sum u_{t}\\
\frac{1}{n^{3/2}}\sum tu_{t}
\end{array}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Example
The denominator 
\begin_inset Formula 
\[
\left(\begin{array}{cc}
1 & \frac{1}{n^{2}}\sum_{t=1}^{n}t\\
\frac{1}{n^{2}}\sum_{t=1}^{n}t & \frac{1}{n^{3}}\sum_{t=1}^{n}t^{2}
\end{array}\right)\to\left(\begin{array}{cc}
1 & \frac{1}{2}\\
\frac{1}{2} & \frac{1}{3}
\end{array}\right)
\]

\end_inset


\end_layout

\begin_layout Example
The numerator is 
\begin_inset Formula 
\[
\left(\begin{array}{c}
\frac{1}{\sqrt{n}}\sum u_{t}\\
\frac{1}{n^{3/2}}\sum tu_{t}
\end{array}\right)=\frac{1}{\sqrt{n}}\sum\left(\begin{array}{c}
1\\
\frac{t}{n}
\end{array}\right)u_{t}=\frac{1}{\sqrt{n}}\sum X_{t}u_{t}
\]

\end_inset

where 
\begin_inset Formula $X_{t}=\left(\begin{array}{c}
1\\
\frac{t}{n}
\end{array}\right)$
\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula 
\[
var\left(\frac{1}{\sqrt{n}}\sum X_{t}u_{t}\right)=\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}X_{i}X_{j}'E\left[u_{i}u_{j}\right]
\]

\end_inset


\end_layout

\begin_layout Example
In the special case when 
\begin_inset Formula $u_{i}$
\end_inset

 is a white noise,
\begin_inset Formula 
\begin{align*}
var\left(\frac{1}{\sqrt{n}}\sum X_{t}u_{t}\right) & =\left(\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}X_{i}X_{j}'\right)\sigma^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}\left(\begin{array}{cc}
1 & \frac{t}{n}\\
\frac{t}{n} & \frac{t^{2}}{n^{2}}
\end{array}\right)\sigma^{2}\overset{d}{\to}\left(\begin{array}{cc}
1 & \frac{1}{2}\\
\frac{1}{2} & \frac{1}{3}
\end{array}\right)\sigma^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 Mar 21, 2023.
   Transcribed by Shu Shen.}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
