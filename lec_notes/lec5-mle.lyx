#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\bullet 1 0 9 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Maximum Likelihood Estimation
\end_layout

\begin_layout Section
Parametric Model
\end_layout

\begin_layout Standard
A parametric model is a complete specification of the distribution.
 Once the parameter is given, the distribution function is determined.
 Instead, a semiparametric model only gives a few features rather than a
 complete description of the distribution.
\end_layout

\begin_layout Example
Semiparametric model: If we know 
\begin_inset Formula $X\sim i.i.d.\left(\mu,\sigma^{2}\right)$
\end_inset

, we can estimate 
\begin_inset Formula $\mu,\sigma^{2}$
\end_inset

 by method of moments.
\end_layout

\begin_layout Example
Parametric model: If we assume 
\begin_inset Formula $X\sim N\left(\mu,\sigma^{2}\right)$
\end_inset

, we can estimate 
\begin_inset Formula $\mu,\sigma^{2}$
\end_inset

 by MLE.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
Conditional model: the conditioning variable can be viewed as if it is fixed
 and the randomness comes from the error term only.
\begin_inset Formula 
\[
y=X'\beta+\varepsilon
\]

\end_inset


\begin_inset Formula $x$
\end_inset

 is the conditional variable.
 The condition 
\begin_inset Formula $E\left(\varepsilon|X\right)=0$
\end_inset

 together with a full rank 
\begin_inset Formula $E\left[XX'\right]$
\end_inset

can help to identify 
\begin_inset Formula $\beta$
\end_inset

.
 This is semiparametric model.
 However, if we assume 
\begin_inset Formula $f\left(\varepsilon\mid X\right)\sim N\left(0,\sigma^{2}\right)$
\end_inset

, then conditional parametric model as it completely describes 
\begin_inset Formula $f\left(y\mid X\right)$
\end_inset

 and it becomes a conditional parametric model.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Definition

\series bold
Parametric model
\series default
.
 The distribution of the data 
\begin_inset Formula $\left(x_{1},...,x_{n}\right)$
\end_inset

 is known up to a finite dimensional parameter.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\Theta$
\end_inset

 be the parameter space a researcher specifies.
 
\end_layout

\begin_layout Definition
A model is 
\series bold
correctly specified
\series default
, if the true DGP is 
\begin_inset Formula $f\left(X\mid\theta_{0}\right)$
\end_inset

 for some 
\begin_inset Formula $\theta_{0}\in\Theta$
\end_inset

.
 Otherwise, the model is 
\series bold
misspecified
\series default
.
\end_layout

\begin_layout Section
Likelihood
\end_layout

\begin_layout Standard
In this chapter we will mostly talk about unconditional models.
 The results can be carried over to conditional models.
 To keep the setting simple, let 
\begin_inset Formula $(X_{1},\ldots,X_{n})$
\end_inset

 be i.i.d.
 The 
\series bold
likelihood
\series default
 of the sample is 
\begin_inset Formula $\prod_{i=1}^{n}f\left(X_{i}\mid\theta_{0}\right)$
\end_inset

.
 The 
\series bold
log-likelihood
\series default
 is 
\begin_inset Formula 
\[
\ell_{n}\left(\theta\right)=\frac{1}{n}\sum_{i=1}^{n}\log f\left(X_{i}\mid\theta\right).
\]

\end_inset

Here, we put 
\begin_inset Formula $1/n$
\end_inset

 to average the log-likelihood.
 This scaling factor does not change the estimation at all.
 
\end_layout

\begin_layout Standard
In practice, we work with the log-likelihood, which is more convenient.
 the MLE estimator is defined as 
\begin_inset Formula 
\[
\hat{\theta}=\arg\max_{\theta\in\varTheta}\ell_{n}\left(\theta\right).
\]

\end_inset

To justify the likelihood principle, consider the population version of
 the 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\ell\left(\theta\right)=E\left[\log f\left(X\mid\theta\right)\right]
\]

\end_inset


\end_layout

\begin_layout Theorem
When model is correctly specified, 
\begin_inset Formula $\theta_{0}$
\end_inset

 is the maximizer.
\end_layout

\begin_layout Proof
Kullback-Leibler distence
\begin_inset Formula 
\begin{align*}
E\left[\log p\left(\theta_{0}\right)\right]-E\left[\log p\left(\theta\right)\right] & =E\left[\log\left(p\left(\theta_{0}\right)/p\left(\theta\right)\right)\right]\\
 & =-E\left[\log\left(p\left(\theta\right)/p\left(\theta_{0}\right)\right)\right]\\
 & \geq-\log E\left[p\left(\theta\right)/p\left(\theta_{0}\right)\right]=0
\end{align*}

\end_inset

where the inequality holds by Jensen's inequality for the convex function
 
\begin_inset Formula $-\log(\cdot)$
\end_inset

.
\end_layout

\begin_layout Section
Score, Hessian, and Information
\end_layout

\begin_layout Standard
Score:
\begin_inset Formula 
\[
\psi_{n}\left(\theta\right)=\sum_{i=1}^{n}\frac{\partial}{\partial\theta}\log f\left(X_{i}\mid\theta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Hessian:
\begin_inset Formula 
\[
\mathscr{H}_{n}\left(\theta\right)=-\sum_{i=1}^{n}\frac{\partial^{2}}{\partial\theta\partial\theta'}\log f\left(X_{i}\mid\theta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Efficient score:
\begin_inset Formula 
\[
\psi_{0}=\frac{\partial}{\partial\theta}\log f\left(X_{i}\mid\theta_{0}\right)
\]

\end_inset


\end_layout

\begin_layout Theorem
If the model is correctly specified, the support of 
\begin_inset Formula $X$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

, and 
\begin_inset Formula $\theta_{0}$
\end_inset

 is in the interior of 
\begin_inset Formula $\Theta$
\end_inset

, then 
\begin_inset Formula $E\left(\psi_{0}\right)=0$
\end_inset

.
\end_layout

\begin_layout Proof
By the Leibniz rule,
\begin_inset Formula 
\[
E\left(\psi_{0}\right)=E\left[\frac{\partial}{\partial\theta}\log f\left(X_{i}\mid\theta_{0}\right)\right]=\frac{\partial}{\partial\theta}E\left[\log f\left(X_{i}\mid\theta_{0}\right)\right]=0
\]

\end_inset

as 
\begin_inset Formula $\theta_{0}$
\end_inset

 is the maximizer in an interior.
\end_layout

\begin_layout Definition
Fisher information matrix:
\begin_inset Formula 
\[
\mathscr{I}_{0}=E\left[\psi_{0}\psi_{0}'\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Definition
Expected Hessian:
\begin_inset Formula 
\[
\mathscr{H}_{0}=-E\left[\frac{\partial^{2}}{\partial\theta\partial\theta'}\log f\left(X\mid\theta_{0}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Theorem
If the model is correctly specified, we have the 
\series bold
information matrix equality
\series default
: 
\begin_inset Formula $\mathscr{I}_{0}=\mathscr{H}_{0}$
\end_inset

.
\end_layout

\begin_layout Proof
Start with Hessian,
\begin_inset Formula 
\begin{align*}
E\left[\frac{\partial^{2}}{\partial\theta\partial\theta'}\log f\left(\theta_{0}\right)\right] & =E\left[\frac{\partial}{\partial\theta}\frac{\partial}{\partial\theta'}\log f\left(\theta_{0}\right)\right]\\
 & =E\left[\frac{\partial}{\partial\theta}\frac{\frac{\partial}{\partial\theta'}f\left(\theta\right)}{f\left(\theta\right)}\big|_{\theta=\theta_{0}}\right]\\
 & =E\left[\frac{\frac{\partial^{2}}{\partial\theta\partial\theta'}f\left(\theta\right)}{f\left(\theta_{0}\right)}\right]+E\left[\frac{\frac{\partial}{\partial\theta}f\left(\theta\right)\frac{\partial}{\partial\theta'}f\left(\theta\right)}{f^{2}\left(\theta_{0}\right)}\right].
\end{align*}

\end_inset

The first term:
\begin_inset Formula 
\[
E\left[\frac{\frac{\partial^{2}}{\partial\theta\partial\theta'}f\left(\theta\right)}{f\left(\theta_{0}\right)}\right]=\int\frac{\frac{\partial^{2}}{\partial\theta\partial\theta'}f\left(\theta\right)}{f\left(\theta_{0}\right)}f\left(\theta_{0}\right)dx=\int\frac{\partial^{2}}{\partial\theta\partial\theta'}f\left(\theta\right)dx=\frac{\partial^{2}}{\partial\theta\partial\theta'}\int f\left(\theta\right)dx=\frac{\partial^{2}}{\partial\theta\partial\theta'}1=0.
\]

\end_inset

The second term:
\begin_inset Formula 
\[
E\left[\frac{\frac{\partial}{\partial\theta}f\left(\theta\right)\frac{\partial}{\partial\theta'}f\left(\theta\right)}{f^{2}\left(\theta_{0}\right)}\right]=E\left[\frac{\partial}{\partial\theta}\log f\left(\theta_{0}\right)\frac{\partial}{\partial\theta'}\log f\left(\theta_{0}\right)\right]=E\left[\psi_{0}\psi_{0}'\right].
\]

\end_inset


\end_layout

\begin_layout Standard
Notice that the information matrix equality holds only when the model is
 correctly specified.
 It fails when the model is misspecified.
 
\end_layout

\begin_layout Section
Cramér-Rao Lower Bound
\end_layout

\begin_layout Theorem
Suppose the model is correctly specified, the support of 
\begin_inset Formula $X$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

, and 
\begin_inset Formula $\theta_{0}$
\end_inset

 is in the interior of 
\begin_inset Formula $\Theta$
\end_inset

.
 If 
\begin_inset Formula $\widetilde{\theta}$
\end_inset

 is unbiased estimator, then 
\begin_inset Formula $var\left(\widetilde{\theta}\right)\geq\left(n\mathscr{I}_{0}\right)^{-1}$
\end_inset

.
\end_layout

\begin_layout Proof
Because of unbiasedness,
\begin_inset Formula 
\[
\theta=E_{\theta}\left[\widetilde{\theta}\right]=\int\widetilde{\theta}f\left(\mathbf{X}\mid\theta\right)d\mathbf{x}
\]

\end_inset

for any 
\begin_inset Formula $\theta\in\Theta$
\end_inset

.
 
\begin_inset Formula $\mathbf{X}$
\end_inset

 here is for the entire sample, 
\begin_inset Formula $f\left(\mathbf{X}\mid\theta\right)=f\left(X_{1},...,X_{n}\mid\theta\right)=\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)$
\end_inset

.
 Take derivative at the two sides.
 The LHS is 
\begin_inset Formula 
\[
\frac{\partial\theta}{\partial\theta'}=\mathbf{I}_{p}
\]

\end_inset

.
 The RHS:
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\theta'}\int\widetilde{\theta}f\left(\mathbf{X}\mid\theta\right)d\mathbf{x} & =\int\widetilde{\theta}\frac{\partial}{\partial\theta'}f\left(\mathbf{X}\mid\theta\right)d\mathbf{x}\\
 & =\int\widetilde{\theta}\frac{\frac{\partial}{\partial\theta'}f\left(\mathbf{X}\mid\theta\right)}{f\left(\mathbf{X}\mid\theta\right)}f\left(\mathbf{X}\mid\theta\right)d\mathbf{x}\\
 & =\int\widetilde{\theta}\frac{\partial}{\partial\theta'}\log f\left(\mathbf{X}\mid\theta\right)f\left(\mathbf{X}\mid\theta\right)d\mathbf{x}\\
 & =\int\widetilde{\theta}\psi_{n}\left(\theta\right)f\left(\mathbf{X}\mid\theta\right)d\mathbf{x}
\end{align*}

\end_inset

Evaluate at the true 
\begin_inset Formula $\theta_{0}$
\end_inset

, and due to i.i.d.
 data
\begin_inset Formula 
\[
\mathbf{I}_{p}=\int\widetilde{\theta}\psi_{n}(\theta_{0})f\left(\mathbf{X}\mid\theta_{0}\right)d\mathbf{x}=E\left[\widetilde{\theta}\psi_{n}(\theta_{0})\right]=E\left[\left(\widetilde{\theta}-\theta_{0}\right)\psi_{n}(\theta_{0})\right]
\]

\end_inset

where the last equality holds by 
\begin_inset Formula $E\left[\theta_{0}\psi_{n}(\theta_{0})\right]=\theta_{0}E\left[\psi_{n}(\theta_{0})\right]=\theta_{0}E\left[n\psi_{0}\right]=0$
\end_inset

.
 We thus have
\begin_inset Formula 
\[
var\left(\begin{array}{c}
\widetilde{\theta}-\theta_{0}\\
\psi_{n}(\theta_{0})
\end{array}\right)=\left[\begin{array}{cc}
\boldsymbol{V} & \mathbf{I}_{p}\\
\mathbf{I}_{p} & n\mathscr{I}_{0}
\end{array}\right].
\]

\end_inset

Pre- and post-multiply 
\begin_inset Formula $\left[\begin{array}{cc}
\boldsymbol{I}_{p} & -\left(n\mathscr{I}_{0}\right)^{-1}\end{array}\right]$
\end_inset

, we have
\begin_inset Formula 
\[
\left[\begin{array}{cc}
\boldsymbol{I}_{p} & -\left(n\mathscr{I}_{0}\right)^{-1}\end{array}\right]\left[\begin{array}{cc}
\boldsymbol{V} & \boldsymbol{I}_{p}\\
\boldsymbol{I}_{p} & n\mathscr{I}_{0}
\end{array}\right]\left[\begin{array}{c}
\boldsymbol{I}_{p}\\
-\left(n\mathscr{I}_{0}\right)^{-1}
\end{array}\right]=\boldsymbol{V}-\left(n\mathscr{I}_{0}\right)^{-1}\geq0.
\]

\end_inset


\end_layout

\begin_layout Standard
The Cramér-Rao Lower Bound is a lower bound.
 It may not reachable.
 When it is reached, an estimator is 
\series bold
Cramér-Rao efficient
\series default
 if it is unbiased and the variance is 
\begin_inset Formula $\left(n\mathscr{I}_{0}\right)^{-1}$
\end_inset

.
 
\end_layout

\begin_layout Example
Normal distribution: Let 
\begin_inset Formula $\gamma=\sigma^{2}$
\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula 
\[
\log\ell_{n}\left(X\mid\mu,\sigma^{2}\right)=-\frac{n}{2}\log\gamma-\frac{n}{2}\log\pi-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula 
\[
\psi_{n}\left(\mu,\sigma^{2}\right)=\begin{cases}
\frac{1}{\gamma}\sum_{i=1}^{n}\left(X_{i}-\mu\right)\\
-\frac{n}{2\gamma}+\frac{1}{2\gamma^{2}}\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula 
\[
\mathscr{H}_{n}\left(\mu,\sigma^{2}\right)=\left[\begin{array}{cc}
\frac{n}{\gamma} & \frac{1}{2\gamma^{2}}\sum_{i=1}^{n}\left(X_{i}-\mu\right)\\
\frac{1}{2\gamma^{2}}\sum_{i=1}^{n}\left(X_{i}-\mu\right) & -\frac{n}{2\gamma^{2}}+\frac{1}{\gamma^{3}}\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Example
Expected Hessian:
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
E\left[\mathscr{H}_{n}\left(\mu,\sigma^{2}\right)\right]=\left[\begin{array}{cc}
\frac{n}{\gamma} & 0\\
0 & \frac{n}{2\gamma^{2}}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Example
Take inverse:
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
\left[\begin{array}{cc}
\frac{\gamma}{n} & 0\\
0 & 2\frac{\gamma^{2}}{n}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Example
This is the lower bound.
\end_layout

\begin_layout Example
Check: 
\end_layout

\begin_layout Example
the sample mean: 
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)=\frac{\sigma^{2}}{n}
\]

\end_inset


\end_layout

\begin_layout Example
The sample mean is Cramér-Rao efficient.
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
s_{n}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\frac{1}{n-1}X'\left(I-\frac{1}{n}1_{n}1_{n}'\right)X
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula $E\left(S_{n}^{2}\right)=\sigma^{2}$
\end_inset

 is unbiased
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
\left(n-1\right)\frac{s_{n}^{2}}{\sigma^{2}}=\left(\frac{X}{\sigma}\right)'\left(I-\frac{1}{n}1_{n}1_{n}'\right)\left(\frac{X}{\sigma}\right)\sim\chi^{2}\left(n-1\right)
\]

\end_inset


\end_layout

\begin_layout Example
So,
\begin_inset Formula 
\[
s_{n}^{2}=\frac{\chi^{2}\left(n-1\right)}{n-1}\sigma^{2}
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula 
\[
var\left(s_{n}^{2}\right)=\frac{\sigma^{4}}{\left(n-1\right)^{2}}2\left(n-1\right)=\frac{2\sigma^{4}}{n-1}>\frac{2\sigma^{4}}{n}
\]

\end_inset


\end_layout

\begin_layout Example
Does not satisfy Cramér-Rao efficient.
\end_layout

\begin_layout Section
Asymptotic Normality
\end_layout

\begin_layout Standard
MLE is a special case of m-estimator.
 Under regularity conditions, 
\begin_inset Formula $\hat{\theta}\stackrel{p}{\to}\theta_{0}$
\end_inset

, and asymptotically normal:
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\mathscr{H}_{0}^{-1}\mathscr{I}_{0}\mathscr{H}_{0}^{-1}\right)
\]

\end_inset

When the information equality hods, the asymptotic variance is simplified
 as 
\begin_inset Formula $\mathscr{\mathscr{I}}_{0}^{-1}\mathscr{I}_{0}\mathscr{\mathscr{I}}_{0}^{-1}=\mathscr{\mathscr{I}}_{0}^{-1}$
\end_inset

, and thus 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\mathscr{I}_{0}^{-1}\right).
\]

\end_inset

In other words, it achieves asymptotic efficiency.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
For any variable,
\begin_inset Formula 
\[
\sqrt{n}\left(\tilde{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Sigma\right)
\]

\end_inset


\end_layout

\begin_layout Plain Layout
We say it is asymptotically unbiased.
\end_layout

\begin_layout Plain Layout
Then 
\begin_inset Formula $\mathscr{I}_{0}^{-1}$
\end_inset

 is the asymptotic Cramér-Rao efficiency bound.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Caveat:
\end_layout

\begin_layout Enumerate
need correct specification
\end_layout

\begin_layout Enumerate
the comparison is restricted to asymptotically unbiased estimator.
 There are biased estimators with better overall performance.
\end_layout

\begin_layout Section
Kullback-Leibler Divergence
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
KLIC\left(f,g\right)=\int f\left(x\right)\log\frac{f\left(x\right)}{g\left(x\right)}dx
\]

\end_inset


\end_layout

\begin_layout Standard
Properties:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $KLIC\left(f,f\right)=0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $KLIC\left(f,g\right)\geq0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $f=\arg\min_{g}KLIC\left(f,g\right)$
\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $f\left(x\right)=f\left(x\mid\theta\right)$
\end_inset

 is a parametric family
\begin_inset Formula 
\[
\theta_{0}=\arg\min_{\theta\in\varTheta}KLIC\left(f,f_{\theta}\right)
\]

\end_inset

which is correctly specified model.
\end_layout

\begin_layout Standard
Pseudo-true parameter:
\begin_inset Formula 
\[
\theta_{0}=\arg\min_{\theta\in\varTheta}KLIC\left(f,f_{\theta}\right)
\]

\end_inset

which is misspecified model.
\end_layout

\begin_layout Standard
KLIC is the distance measure of any two distributions.
\begin_inset Formula 
\begin{align*}
KLIC\left(f,f_{\theta}\right) & =\int f\left(x\right)\log f\left(x\right)dx-\int f\left(x\right)\log f\left(x\mid\theta\right)dx\\
 & =\int f\left(x\right)\log f\left(x\right)dx-E\left[\log f\left(x\mid\theta\right)\right]\\
 & =\int f\left(x\right)\log f\left(x\right)dx-\ell\left(\theta\right)
\end{align*}

\end_inset

the pseudo-true value
\begin_inset Formula 
\[
\theta^{*}=\arg\max_{\theta\in\varTheta}\ell\left(\theta\right)
\]

\end_inset

The information equality was proved under correct specification.
 When the model is misspecified,
\begin_inset Formula 
\[
E\left[S\left(\theta^{*}\right)S\left(\theta^{*}\right)'\right]\neq E\left[\frac{\partial^{2}}{\partial\theta\partial\theta'}\log f\left(\theta^{*}\right)\right].
\]

\end_inset

As a result, we will have a sandwich-form asymptotic variance in 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{*}\right)\stackrel{d}{\to}N\left(0,\mathscr{H}_{*}^{-1}\mathscr{I}_{*}\mathscr{H}_{*}^{-1}\right)
\]

\end_inset

understand that 
\begin_inset Formula $\mathscr{I}_{*}$
\end_inset

 and 
\begin_inset Formula $\mathscr{H}_{*}$
\end_inset

 are evaluated at the pseudo-true value.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 Feb 14, 2023.
   Transcribed by Shu Shen.}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
